\section{Vorbetrachtungen}
In diesem Projekt sollte untersucht werden, wie die Rechenleistung der Computer
im PC-Pool des Instituts für Physik der Humboldt-Universität zu Berlin von
verschiedenen Parametern abhängt. Als Benchmark-Methode wurde \texttt{daxpy}
verwendet. Dabei handelt es sich um ein Verfahren, das aus zwei Vektoren in einer \for-Schleife
komponentenweise eine Summe bildet, wobei an einen der beiden Werte eine
beliebige Zahl multipliziert wird. Somit treten für jede Komponente zwei
Operationen auf, eine Addition und eine Multiplikation. Die Daten sind hierbei 
unabhängig voneinander.

Die Prozessoren sind Intel-Quadcore-CPUs des Typs „Q9450.“ Das Betriebssystem ist
ein 64bit-System.

Die Parameter, von denen die Rechenleistung abhängt und die daraufhin untersucht
wurden, sind zum einen die Vektorlänge, zum anderen die Schrittlänge („Stride“),
mit der die \for-Schleife durchschritten wird,
sowie auch die konkrete Wahl des Compilers und der Optimierung durch die 
Compiler. Die Vektorlänge wurde von 100 bis $10^7$ variiert, die Schrittlänge 
von 1 bis 64. Die verwendeten Compiler sind der \gcc, der \icc sowie der \pgcc.
Weiterhin wurde das Optimierungslevel verändert, mit dem die Compiler den Code 
überprüfen und optimieren. Wenn man beim kompilieren keinen Parameter übergibt, 
werden die Standardeinstellungen des jeweiligen Compilers verwendet, bei der
Verwendung des Parameters \texttt{-o0} sollte keinerlei Optimierung durchgefürt
werden, bei einem Parameter \texttt{-o3} hingegen sollte der Code für die
Ausführung in der Art angepasst werden, dass das Programm schnell läuft.

Im Folgenden werden die sich ergebenden Plots sowie der Quelltext unserer Programme
gezeigt.



\section{Auswertung}
\subsection{Einfluss der Vektorlänge $n$}
Zunächst wird der Einfluss der Vektorlänge studiert. Unabhängig vom Compiler, von
der Optimierungsstufe und von der Schrittlänge nimmt die Rechenleistung tendenziell
ab, wenn man die Länge der Vektoren vergrößert. Dies geschieht meist sprunghaft,
also nicht kontinuierlich. Dafür gibt es mehrere Ursachen. Zum einen kann beobachtet
werden, dass es (zum Beispiel in \fref{gcc_o3}) zwischen $n=\num{1e5}$ und
$n=\num{1e6}$ zu einem Leistungseinbruch um einen Faktor von ca. drei bis vier kommt.
Dies lässt sich über die Speichergrößen der beteiligten Vektoren begründen. Wenn 
man zwei Vektoren betrachtet, die die Länge \num{1e6} besitzen und mit Zahlen vom Typ
\texttt{double} gefüllt sind, so berechnet sich die benötigte Größe zu insgesamt
$2\cdot \num{64e6}\mbox{~bit}\approx \SI{15.3}{\mega\byte}$. Die Größe des Caches 
beträgt \SI{6}{\mega\byte}, sodass ab etwa einer Vektorlänge von schätzungsweise \num{0.5e6}     die 
Vektoren nicht mehr in den Cache passen. Daher ist das Betriebssystem gezwungen,
die Daten aus dem Arbeitsspeicher zu laden, was deutlich langsamer ist als das Laden
aus dem Cache. Für noch größere Längen bleibt die Leistung auf dem gleichen Niveau.
Man kann für die meisten Plots einen weiteren Leistungseinbruch beobachten, dieses
mal zwischen $n=1000$ und $n=10000$. Dies lässt sich dadurch begründen, dass neben
dem Level-2-Cache (L2-Cache) ein kleinerer Level-1-Cache (L1-Cache) existiert, zu
dem die Daten aus dem L2-Cache transportiert werden müssen. Hierbei ist die Bandbreite
begrenzt, sodass sich dies auf die effektive Leistung auswirken kann. Dabei ist auch
die Schrittweite von Bedeutung, da die Größe des L1-Caches sehr limitiert ist (einige
Kilobyte bis einige hundert Kilobyte) und daher nur wesentlich kleinere Teile der
Vektoren in diesen Cache passen und bei einer größeren Schrittweite öfter nachgeladen
werden muss. Dies kann zu dem beobachteten Effekt führen. Um die Größenordnung des
L1-Caches zu verifizieren, wird kurz überschlagen, welchem Speicherbedarf die
genannten Vektorlängen entsprechen: $$2\cdot64\cdot \SI{1e4}{\bit} \approx 
\SI{156}{\kibi\byte},$$ was mit den Erwartungen übereinstimmt. Es ist also sehr
wahrscheinlich, dass es sich bei diesem Einbruch um einen Bandbreiteneffekt zwischen
L2- und L1-Cache handelt.





% - gcc_o:	max. leistung: ca. 360
% 		anfang: alles ähnlich, 100 kaum aussagekräftig, ab 1e6: stride > 1 deutlich schlechter als stride = 1, größerer stride -> schlechterer wert, stimmt für 16 bis 64 überein
% - gcc_o0:	= _o
% - gcc_o3:	rechenleistung bis faktor 3 besser (auf max. 1300), ab n=1e4: wieder deutlicher einfluss von stride!=1, erniedrigung 
% 		um ca. 200, bis min. 400, ab n=1e6: wie _o0
% 		erste stufe: n=1e3 entspricht 7.8kByte, cache size: 6144kB -> ab n=1e6 passt der vektor nicht mehr in den cache -> paging(?), clflush size: 64, cache_alignment: 64
% 		stride 1 -> viele sinnvolle daten in cache -> ideales nachladen
% 		erste stufe: schritt so groß, dass das geladene stück des vektors nur für eine zahl genutzt werden kann -> register komplett neu füllen -> man sieht hier die beschränkung durch bandbreite zwischen cache und register + pipeline-leerlauf; register: 64bit -> kann nicht der grund sein!! dann könnte die erste stufe die beschränkung durch l1 sein, je größer die schrittweite ist, desto mehr muss von l2 in l1 geladen werden. daher könnte es zu den stride-abhängigen stufen kommen
% 
% - stride 1, o3:	alle compiler etwa gleich gut
% 
% 
% 
% 
% 
% 
% 
% 
% vektorlänge n, double-größe: 8byte->64bit
% for entspricht if -> pipeline läuft leer, cache+register evtl neu laden
% 
% es müssen zwei vektoren reinpassen, man kann den genauen punkt hier nicht sehen
% annahme : compiler vektorisieren eigenständig (bei o3), was hier gut möglich ist (keine abhängigkeiten)
% 
% pgcc optimiert trotz o0-anweisung
% 
% 
% fehler in berechnung der rechenleistung: reale zeit, keine cpu-zeit