\section{Vorbetrachtungen}
In diesem Projekt sollte untersucht werden, wie die Rechenleistung der Computer
im PC-Pool des Instituts für Physik der Humboldt-Universität zu Berlin von
verschiedenen Parametern abhängt. Als Benchmark-Methode wurde \texttt{daxpy}
verwendet. Dabei handelt es sich um ein Verfahren, das aus zwei Vektoren in einer \for-Schleife
komponentenweise eine Summe bildet, wobei an einen der beiden Werte eine
beliebige Zahl multipliziert wird. Somit treten für jede Komponente zwei
Operationen auf, eine Addition und eine Multiplikation. Die Daten sind hierbei 
unabhängig voneinander. Die Rechnungen wurden teils mehrfach wiederholt und
gemittelt.

Die Prozessoren sind Intel-Quadcore-CPUs des Typs „Q9450.“ Das Betriebssystem ist
ein 64bit-System.

Die Parameter, von denen die Rechenleistung abhängt und die daraufhin untersucht
wurden, sind zum einen die Vektorlänge, zum anderen die Schrittlänge („Stride“),
mit der die \for-Schleife durchschritten wird,
sowie auch die konkrete Wahl des Compilers und der Optimierung durch die 
Compiler. Die Vektorlänge wurde von 100 bis $10^7$ variiert, die Schrittlänge 
von 1 bis 64. Die verwendeten Compiler sind der \gcc, der \icc sowie der \pgcc.
Weiterhin wurde das Optimierungslevel verändert, mit dem die Compiler den Code 
überprüfen und optimieren. Wenn man beim kompilieren keinen Parameter übergibt, 
werden die Standardeinstellungen des jeweiligen Compilers verwendet, bei der
Verwendung des Parameters \texttt{-o0} sollte keinerlei Optimierung durchgefürt
werden, bei einem Parameter \texttt{-o3} hingegen sollte der Code für die
Ausführung in der Art angepasst werden, dass das Programm schnell läuft.

Die sich ergebenden Plots sowie der Quelltext unserer Programme werden in 
gezeigt.



\section{Auswertung}
\subsection{Einfluss der Vektorlänge $n$}
Zunächst wird der Einfluss der Vektorlänge studiert. Unabhängig vom Compiler, von
der Optimierungsstufe und von der Schrittlänge nimmt die Rechenleistung tendenziell
ab, wenn man die Länge der Vektoren vergrößert. Dies geschieht meist sprunghaft,
also nicht kontinuierlich. Dafür gibt es mehrere Ursachen. Zum einen kann beobachtet
werden, dass es (zum Beispiel in \fref{gcc_o3}) zwischen $n=\num{1e5}$ und
$n=\num{1e6}$ zu einem Leistungseinbruch um einen Faktor von ca. drei bis vier kommt.
Dies lässt sich über die Speichergrößen der beteiligten Vektoren begründen. Wenn 
man zwei Vektoren betrachtet, die die Länge \num{1e6} besitzen und mit Zahlen vom Typ
\texttt{double} gefüllt sind, so berechnet sich die benötigte Größe zu insgesamt
$2\cdot \num{64e6}\mbox{~bit}\approx \SI{15.3}{\mega\byte}$. Die Größe des Caches 
beträgt \SI{6}{\mega\byte}, sodass ab etwa einer Vektorlänge von schätzungsweise \num{0.5e6} die 
Vektoren nicht mehr in den Cache passen. Daher ist das Betriebssystem gezwungen,
die Daten aus dem Arbeitsspeicher zu laden, was deutlich langsamer ist als das Laden
aus dem Cache. Für noch größere Längen bleibt die Leistung auf dem gleichen Niveau.

Man kann für die meisten Plots einen weiteren Leistungseinbruch beobachten, dieses
mal zwischen $n=1000$ und $n=10000$. Dies lässt sich dadurch begründen, dass neben
dem Level-2-Cache (L2-Cache) ein kleinerer Level-1-Cache (L1-Cache) existiert, zu
dem die Daten aus dem L2-Cache transportiert werden müssen. Hierbei ist die Bandbreite
begrenzt, sodass sich dies auf die effektive Leistung auswirken kann. Dabei ist auch
die Schrittweite von Bedeutung, da die Größe des L1-Caches sehr limitiert ist (einige
Kilobyte bis einige hundert Kilobyte) und daher nur wesentlich kleinere Teile der
Vektoren in diesen Cache passen und bei einer größeren Schrittweite öfter nachgeladen
werden muss. Dies kann zu dem beobachteten Effekt führen. Um die Größenordnung des
L1-Caches zu verifizieren, wird kurz überschlagen, welchem Speicherbedarf die
genannten Vektorlängen entsprechen: $2\cdot\SI{64e4}{\bit} \approx 
\SI{156}{\kibi\byte}$, was mit den Erwartungen übereinstimmt. Es ist also sehr
wahrscheinlich, dass es sich bei diesem Einbruch um einen Bandbreiteneffekt zwischen
L2- und L1-Cache handelt.

Für kleine Vektorlängen wird in die maximale Leistung erreicht, die zwischen 1300
und \SI{1400}{MFlops} liegt (je nach Compiler). Hier die theoretisch maximal mögliche 
Leistung wird auch hier nicht erreicht, da auch hier, selbst im Falle kleiner 
Schrittlängen, der L1-Cache oft aktualisiert werden muss, was die Leistung erniedrigt.
Weitere Begrenzung können daraus resultieren, dass nach jeder \for-Schleife eine 
Art \texttt{if}-Abfrage durchgefürt wird, ob die Schleife weiter ausgeführt werden
soll. An solchen Stellen kann es vorkommen, dass die Pipeline in der CPU leerläuft,
was ebenfalls zu Geschwindigkeitseinbußen führen kann.

Weiterhin soll angemerkt werden, dass die Ergebnisse bei $n=100$ nicht sehr
aussagekräftig sind, speziell für große Schrittlängen, da hier sehr wenige Rechenschritte
durchgefürt werden. Hierbei kann es zu starken Schwankungen kommen, sodass sich in
der weiteren Auswertung auf größere Längen konzentriert wird.


\subsection{Einfluss der Schrittweite}
An dieser Stelle soll der Einfluss des Strides genauer untersucht werden. Wie bereits
festgestellt werden konnte, sind kleine Schrittlängen vorteilhaft (z.B. \fref{gcc_o3}
oder \fref{pgcc_o}), da die in die verschiedenen Cache-Level geladenen Daten am
effektivsten verwendet werden. Allerdings ist dieser Effekt vom verwendeten Compiler
abhängig und auch von der Optimierungsstufe, was im nächsten Kapitel besprochen wird.
Bei der Optimierungsoption \texttt{-o3} treten bei allen Compilern große Unterschiede
zwischen den zu den verschiedenen Strides gehörenden Kurven auf. Daher soll hier kurz 
der Grund für die zum Teil starke Abhängigkeit erläutert werden.

Bei kleinen Vektoren könnte es sein, dass die Compiler die Schleifen gut parallelisieren
(beispielsweise über das sogenannte „Entrollen“) und somit die Pipeline nie
leerläuft, was somit unabhängig von der Schrittweite zu guten Leistungen führt. Bei
größeren Vektoren ($n\ge\num{e4}$) kommt der Effekt des öfteren Nachladens vom L2- in
den L1-Cache zum tragen, was dazu führt, dass die Schrittweiten zwei, vier und acht 
das Programm sukzessive langsamer werden lassen. Bei noch größeren Strides aber bleibt
die Leistung konstant, was so zu verstehen ist, dass für jeden neuen Schleifendurchlauf
neue Daten geladen werden müssen. Hier ist also die Datenmenge, die in eine Cachezeile 
passt, so groß, dass maximal 16 \texttt{double}-Zahlen darin Platz finden.

Für noch größere Vektorlängen ($n\ge \num{e6}$) dominiert das Nachladen aus dem
Arbeitsspeicher die Leistung, Doch auch hier liegen die zum Stride eins (und teilweise
auch zwei) gehörenden Kurven über den anderen. Dies lässt sich wiederum durch die bessere
Nutzung der Daten erklären, da für kleine Schrittweiten seltener aus dem Arbeitsspeicher
nachgeladen werden muss. Für größere Schrittlängen verschwindet dieser Effekt.





% - gcc_o:	max. leistung: ca. 360
% 		anfang: alles ähnlich, 100 kaum aussagekräftig, ab 1e6: stride > 1 deutlich schlechter als stride = 1, größerer stride -> schlechterer wert, stimmt für 16 bis 64 überein
% - gcc_o0:	= _o
% - gcc_o3:	rechenleistung bis faktor 3 besser (auf max. 1300), ab n=1e4: wieder deutlicher einfluss von stride!=1, erniedrigung 
% 		um ca. 200, bis min. 400, ab n=1e6: wie _o0
% 		erste stufe: n=1e3 entspricht 7.8kByte, cache size: 6144kB -> ab n=1e6 passt der vektor nicht mehr in den cache -> paging(?), clflush size: 64, cache_alignment: 64
% 		stride 1 -> viele sinnvolle daten in cache -> ideales nachladen
% 		erste stufe: schritt so groß, dass das geladene stück des vektors nur für eine zahl genutzt werden kann -> register komplett neu füllen -> man sieht hier die beschränkung durch bandbreite zwischen cache und register + pipeline-leerlauf; register: 64bit -> kann nicht der grund sein!! dann könnte die erste stufe die beschränkung durch l1 sein, je größer die schrittweite ist, desto mehr muss von l2 in l1 geladen werden. daher könnte es zu den stride-abhängigen stufen kommen
% 
% - stride 1, o3:	alle compiler etwa gleich gut
% 
% 
% 
% 
% 
% 
% 
% 
% vektorlänge n, double-größe: 8byte->64bit
% for entspricht if -> pipeline läuft leer, cache+register evtl neu laden
% 
% es müssen zwei vektoren reinpassen, man kann den genauen punkt hier nicht sehen
% annahme : compiler vektorisieren eigenständig (bei o3), was hier gut möglich ist (keine abhängigkeiten)
% 
% pgcc optimiert trotz o0-anweisung
% 
% 
% fehler in berechnung der rechenleistung: reale zeit, keine cpu-zeit

\section{Vergleich der Compiler}
Zur Durchführung der Tests wurden die Compiler
\begin{itemize}
 \item \gcc Version 4.3.2 (Debian 4.3.2-1.1) 
 \item \pgcc Version 10.9-0 64-bit target on x86-64 Linux -tp penryn-64 
 \item \icc Version 11.1 20091012
\end{itemize}
verwendet. In \fref{s01_o0} und \fref{s64_o0} werden die Compiler miteinander
verglichen. Der Compiler \pgcc scheint den anderen beiden Compilern überlegen
zu sein. Doch lässt sich in \fref{step4_pgcc} erkennen, dass beim \pgcc den
Optimierungsleveln andere Standardeinstellungen zu Grunde liegen, wodurch ein
aussagekräftiger Vergleich an dieser Stelle nicht möglich wird. \icc und \gcc
liefern ansonsten ungefähr die gleiche Leistung. Bei sehr hohen Vektorlängen
von \num{1e6} und höher versagt die Optimierung des \pgcc, so dass alle Compiler
eine gleiche Leistung ermöglichen.

Interessant ist in \fref{s64_o0}, dass ab einer Vektorlänge von \num{1e2} die
Leistung zunächst steigt. Dies ist womöglich jedoch der ungenügenden Implementierung
des Tests zuzuschreiben. Der Vektor mit Länge 100 wird bei einem Stride von 64
lediglich durch eine Iteration durchlaufen, wodurch die konditionalen Ausdrücke,
Unterprogrammaufrufe, usw. größeren Einfluss haben. Diese Faktoren führen letztlich
zu einer Abnahme der Rechenleistung.

Aus den Abbildungen \fref{step4_gcc} und \fref{step4_icc} kann entnommen werden,
dass der \gcc standardmäßig keine Optimierungen aktiviert, während der \icc ähnlich
wie der \pgcc bei nicht explizit gegebenem Optimierungslevel automatisch Optimierungen
durchführt.
